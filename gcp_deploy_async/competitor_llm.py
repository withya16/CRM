#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê²½ìŸì‚¬ ë‰´ìŠ¤ ê¸°ë°˜ LLM ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ (ë¹„ë™ê¸° ì²˜ë¦¬ ë²„ì „)

1) Google Sheetsì—ì„œ ê²½ìŸì‚¬ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ
2) ê²½ìŸì‚¬ë³„ ê¸°ì‚¬ë“¤ì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ  LLM ë¶„ì„ â†’ íŒŒíŠ¸ë„ˆì‹­ ëª©ë¡ ìƒì„±
3) ê²°ê³¼ë¥¼ Google Sheetsì— ì €ì¥ (ê¸°ì‚¬ ì œëª©ì—ì„œ ë‚ ì§œ ì¶”ì¶œ í¬í•¨)

"""

import pandas as pd
import json
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import sys
import os
import time
import csv
from io import StringIO
from dotenv import load_dotenv
import re
import asyncio
import aiohttp

#  ì¶”ê°€ import (ë ˆì´íŠ¸ë¦¬ë°‹/ë°±ì˜¤í”„)
import random
from collections import deque

# .env íŒŒì¼ ë¡œë“œ (í˜„ì¬ ë””ë ‰í† ë¦¬ ë° ë¶€ëª¨ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°)
from pathlib import Path
env_paths = [
    Path(__file__).parent / '.env',
    Path(__file__).parent.parent / '.env',
]
for env_path in env_paths:
    if env_path.exists():
        load_dotenv(env_path)
        break
else:
    load_dotenv()  # ê¸°ë³¸ ê²½ë¡œì—ì„œë„ ì‹œë„

API_KEY = os.getenv('OPENAI_API_KEY')
API_ENDPOINT = os.getenv('OPENAI_API_ENDPOINT', 'https://api.openai.com/v1/chat/completions')

# credentials.json ê²½ë¡œ ì°¾ê¸° (í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
_script_dir = Path(__file__).parent
_cred_file_default = _script_dir / 'credentials.json'
if not _cred_file_default.exists():
    _cred_file_default = _script_dir.parent / 'credentials.json'
GS_CRED_FILE = os.getenv('GOOGLE_CREDENTIALS_FILE', str(_cred_file_default))
GS_SPREADSHEET_ID = os.getenv('GOOGLE_SPREADSHEET_ID', '1oYJqCNpGAPBwocvM_yjgXqLBUR07h9_GoiGcAFYQsF8')
GS_INPUT_WORKSHEET = os.getenv('GOOGLE_INPUT_WORKSHEET', 'ê²½ìŸì‚¬ ë™í–¥ ë¶„ì„')
GS_OUTPUT_WORKSHEET = os.getenv('GOOGLE_OUTPUT_WORKSHEET', 'ê²½ìŸì‚¬ í˜‘ì—… ê¸°ì—… ë¦¬ìŠ¤íŠ¸')

# LLM ë¶„ì„ ì„¤ì •
ARTICLES_PER_CALL = int(os.getenv("ARTICLES_PER_CALL", "10"))  # ë°°ì¹˜ë‹¹ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ê°’: 10, API ì‚¬ìš©ëŸ‰ ê°ì†Œë¥¼ ìœ„í•´ 5â†’10ìœ¼ë¡œ ì¦ê°€)
MAX_ARTICLE_CONTENT_LENGTH = int(os.getenv("MAX_ARTICLE_CONTENT_LENGTH", "2000"))  # ê¸°ì‚¬ ë³¸ë¬¸ ìµœëŒ€ ê¸¸ì´ (ê¸€ì ìˆ˜, API ì‚¬ìš©ëŸ‰ ê°ì†Œ)

# ë¹„ë™ê¸° ì²˜ë¦¬ ì„¤ì •
MAX_CONCURRENT_REQUESTS = 2  # ë™ì‹œ ìš”ì²­ ìˆ˜ (ì„¸ë§ˆí¬ì–´)
MAX_BATCH_TASKS_IN_FLIGHT = int(os.getenv("MAX_BATCH_TASKS_IN_FLIGHT", str(MAX_CONCURRENT_REQUESTS * 2)))
# â†‘ ë°°ì¹˜ íƒœìŠ¤í¬ë¥¼ í•œ ê²½ìŸì‚¬ì—ì„œ ë™ì‹œì— â€œì‹¤í–‰ ìƒíƒœâ€ë¡œ ìœ ì§€í•  ê°œìˆ˜(ë©”ëª¨ë¦¬/ë²„ìŠ¤íŠ¸ ë°©ì§€)

if not API_KEY:
    raise ValueError("OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env.exampleì„ ì°¸ê³ í•˜ì„¸ìš”.")

# ---------------------------
#  Rate Limit(í•µì‹¬ ìˆ˜ì •)
# ---------------------------
# ê³„ì •/ëª¨ë¸ ì œí•œì´ ë‹¤ë¥´ë¯€ë¡œ envë¡œ ì‰½ê²Œ ì¡°ì ˆ
OPENAI_RPM = int(os.getenv("OPENAI_RPM", "10"))          # ìš”ì²­/ë¶„(ë³´ìˆ˜ì ìœ¼ë¡œ)
OPENAI_TPM = int(os.getenv("OPENAI_TPM", "20000"))       # í† í°/ë¶„(ë³´ìˆ˜ì ìœ¼ë¡œ)
OPENAI_TIMEOUT_SEC = int(os.getenv("OPENAI_TIMEOUT_SEC", "180"))  # LLM ì‘ë‹µ ëŒ€ê¸°(ì´ˆ)

def estimate_tokens(text: str) -> int:
    """
    í† í° ìˆ˜ ëŸ¬í”„ ì¶”ì •.
    ì‹¤ì œ í† í¬ë‚˜ì´ì €ë¥¼ ì“°ë©´ ì •í™•í•˜ì§€ë§Œ, ì—¬ê¸°ì„  ì•ˆì „í•˜ê²Œ 'chars/3'ë¡œ ë„‰ë„‰íˆ ì¡ìŒ.
    â†’ TPM ì´ˆê³¼ë¥¼ ì¤„ì´ëŠ” ëª©ì .
    """
    if not text:
        return 1
    return max(1, len(text) // 3)

class SlidingWindowLimiter:
    """
    window_seconds ë™ì•ˆ cost í•©ì´ capacityë¥¼ ë„˜ì§€ ì•Šë„ë¡ ëŒ€ê¸°ì‹œí‚¤ëŠ” ê°„ë‹¨ ë¦¬ë¯¸í„°.
    RPM/TPM ëª¨ë‘ ë™ì¼ ë¡œì§ìœ¼ë¡œ ì‚¬ìš©.
    """
    def __init__(self, capacity: int, window_seconds: int = 60):
        self.capacity = capacity
        self.window = window_seconds
        self.q = deque()  # (timestamp, cost)
        self.lock = asyncio.Lock()

    async def acquire(self, cost: int = 1):
        while True:
            async with self.lock:
                now = time.monotonic()

                # ë§Œë£Œëœ í•­ëª© ì œê±°
                while self.q and (now - self.q[0][0]) >= self.window:
                    self.q.popleft()

                used = sum(c for _, c in self.q)
                if used + cost <= self.capacity:
                    self.q.append((now, cost))
                    return

                # ë‹¤ìŒ ìŠ¬ë¡¯ì´ ì—´ë¦´ ë•Œê¹Œì§€ ëŒ€ê¸°
                wait = self.window - (now - self.q[0][0])
                wait = max(0.1, wait)

            await asyncio.sleep(wait)

rpm_limiter = SlidingWindowLimiter(OPENAI_RPM, 60)
tpm_limiter = SlidingWindowLimiter(OPENAI_TPM, 60)

# ---------------------------
# ê²½ìŸì‚¬ ë§¤í•‘
# ---------------------------
COMPETITOR_BUSINESS_MAP = {
    "ê¸€ë£¨ì½”í•": "ì›°ë‹¤", "íŒŒìŠ¤íƒ€": "ì›°ë‹¤", "ê¸€ë£¨ì–´íŠ¸": "ì›°ë‹¤",
    "ê¸€ë£¨ì–´íŠ¸(ë‹¥í„°ë‹¤ì´ì–´ë¦¬)": "ì›°ë‹¤", "ë‹¥í„°ë‹¤ì´ì–´ë¦¬": "ì›°ë‹¤",
    "ëˆ”": "ì›°ë‹¤", "ë‹¤ë…¸": "ì›°ë‹¤", "í•„ë¼ì´ì¦ˆ": "ì›°ë‹¤",
    "ë ˆë²¨ìŠ¤": "ì›°ë‹¤", "ì‹œê·¸ë…¸ìŠ¤": "ì›°ë‹¤", "ë‰´íŠ¸ë¦¬ì„¼ìŠ¤": "ì›°ë‹¤", "ë²„íƒ€": "ì›°ë‹¤", "ì• ë‹ˆí•í”ŒëŸ¬ìŠ¤": "ì›°ë‹¤",
    "í™ˆí•": "ì½”ì–´ìš´ë™ì„¼í„°",
    "ë‹¬ë¨": "ëŒ€ì›…í—¬ìŠ¤ì¼€ì–´,ì½”ì–´ìš´ë™ì„¼í„°",
    "íŒŒí¬ë¡œì‰¬ë¦¬ì¡°íŠ¸": "ì„ ë§ˆì„", "ë”ìŠ¤í…Œì´íë§íŒŒí¬": "ì„ ë§ˆì„",
    "ì²­ë¦¬ì›€": "ì„ ë§ˆì„", "ì˜¤ìƒ‰ê·¸ë¦°ì•¼ë“œí˜¸í…”": "ì„ ë§ˆì„", "ê¹Šì€ì‚°ì†ì˜¹ë‹¬ìƒ˜": "ì„ ë§ˆì„",
    "GCì¼€ì–´": "ëŒ€ì›…í—¬ìŠ¤ì¼€ì–´,ë””ì§€í„¸í—¬ìŠ¤ì¼€ì–´",
    "ë·°ë¦¿": "ì‹œì…€", "ë ˆë“œë°¸ëŸ°ìŠ¤": "ì‹œì…€", "SNPE": "ì‹œì…€", "í—¬ìŠ¤ë§¥ìŠ¤": "ëŒ€ì›…í—¬ìŠ¤ì¼€ì–´,ë””ì§€í„¸í—¬ìŠ¤ì¼€ì–´"
}

def get_google_client():
    """Google Sheets í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
    scope = ["https://spreadsheets.google.com/feeds", 'https://www.googleapis.com/auth/drive']
    
    # credentials íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(GS_CRED_FILE):
        raise FileNotFoundError(
            f"Google credentials íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {GS_CRED_FILE}\n"
            f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\n"
            f"íŒŒì¼ ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(GS_CRED_FILE)}"
        )
    
    try:
        creds = ServiceAccountCredentials.from_json_keyfile_name(GS_CRED_FILE, scope)
        return gspread.authorize(creds)
    except Exception as e:
        raise Exception(f"Google í´ë¼ì´ì–¸íŠ¸ ì¸ì¦ ì‹¤íŒ¨ (íŒŒì¼: {GS_CRED_FILE}): {e}")

def get_gsheet_data(spreadsheet_id, worksheet_name):
    """Google Sheets ë°ì´í„°ë¥¼ Pandas DataFrameìœ¼ë¡œ ë¡œë“œ"""
    try:
        client = get_google_client()
        spreadsheet = client.open_by_key(spreadsheet_id)
        worksheet = spreadsheet.worksheet(worksheet_name)
        df = pd.DataFrame(worksheet.get_all_records())

        url_col = None
        for c in df.columns:
            lower = c.lower()
            if lower in ("url", "ë§í¬", "ê¸°ì‚¬url", "ê¸°ì‚¬ url"):
                url_col = c
                break

        required_cols = ['ê²½ìŸì‚¬', 'ì œëª©', 'ë³¸ë¬¸']
        if not all(col in df.columns for col in required_cols):
            print("ì˜¤ë¥˜: ë°ì´í„°ì— 'ê²½ìŸì‚¬', 'ì œëª©', 'ë³¸ë¬¸' ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.", flush=True)
            return None

        cols = required_cols.copy()
        if url_col:
            cols.append(url_col)

        df = df[cols]
        df = df[df['ë³¸ë¬¸'].astype(str).str.len() > 100].reset_index(drop=True)
        return df

    except Exception as e:
        print(f"Google Sheets ë¡œë“œ ì‹¤íŒ¨: {e}", flush=True)
        return None

DATE_PATTERNS = [
    re.compile(r'(\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.)(?:\s|$|[.,])'),
    re.compile(r'(\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.)'),
    re.compile(r'(\d{4}\.\d{1,2}\.\d{1,2}\.)'),
    re.compile(r'(\d{4}\.\s*\d{1,2}\.\s*\d{1,2})(?:\s|$|[.,])'),
    re.compile(r'(\d{4}\.\d{1,2}\.\d{1,2})(?:\s|$|[.,])'),
    re.compile(r'(\d{2}\.\d{1,2}\.\d{1,2})(?:\s|$|[.,]|"|,|$)'),
    re.compile(r'(\d{4}-\d{1,2}-\d{1,2})(?:\s|$|[.,])'),
    re.compile(r'(\d{4}/\d{1,2}/\d{1,2})(?:\s|$|[.,])'),
    re.compile(r'(\d{8})(?:\s|$|[.,])'),
    re.compile(r'(\d{6})(?:\s|$|[.,])'),
]

def normalize_date_to_yy_mm_dd(date_str: str) -> str:
    """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ YY.MM.DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not date_str:
        return ""
    date_str = str(date_str).strip()

    date_patterns = [
        (r'(\d{4})[.\s]+(\d{1,2})[.\s]+(\d{1,2})', True),
        (r'(\d{4})-(\d{1,2})-(\d{1,2})', True),
        (r'(\d{4})/(\d{1,2})/(\d{1,2})', True),
        (r'(\d{2})\.(\d{1,2})\.(\d{1,2})', False),
        (r'^(\d{4})(\d{2})(\d{2})$', True),
        (r'^(\d{2})(\d{2})(\d{2})$', False),
        (r'(\d{4})\s*ë…„\s*(\d{1,2})\s*ì›”\s*(\d{1,2})\s*ì¼', True),
    ]

    for pattern, has_year in date_patterns:
        m = re.search(pattern, date_str)
        if m:
            if has_year:
                year = m.group(1)
                month = m.group(2)
                day = m.group(3)
                yy = year[-2:] if len(year) == 4 else year
            else:
                yy = m.group(1)
                month = m.group(2)
                day = m.group(3)
            return f"{yy}.{int(month):02d}.{int(day):02d}"

    return date_str

def extract_date_from_title(title: str):
    """ì œëª© ëì—ì„œ ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ê³ , ë‚ ì§œê°€ ì œê±°ëœ ë¬¸ìì—´ì„ ë°˜í™˜"""
    if not isinstance(title, str):
        return None, title

    original_title = title
    search_area = original_title[-500:] if len(original_title) > 500 else original_title

    best_match = None
    best_pos = -1

    for pattern in DATE_PATTERNS:
        matches = list(pattern.finditer(search_area))
        if matches:
            m = matches[-1]
            match_pos = len(original_title) - len(search_area) + m.end()
            if match_pos > best_pos:
                best_match = m
                best_pos = match_pos

    if best_match:
        date_str_full = best_match.group(0)
        date_str_group = best_match.group(1)

        new_title = original_title.replace(date_str_full, "", 1).strip()
        new_title = re.sub(r'[.,\s\[\]\(\)\-â€“â€”ï½œ|]+$', '', new_title).strip()

        normalized_date = normalize_date_to_yy_mm_dd(date_str_group)
        return normalized_date, new_title

    return None, original_title

def add_article_dates(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameì— 'ê¸°ì‚¬ ë‚ ì§œ' ì»¬ëŸ¼ ì¶”ê°€ (ì œëª©ì—ì„œ ì¶”ì¶œ)"""
    if "ê·¼ê±° ê¸°ì‚¬ ì œëª©" not in df.columns:
        return df

    titles, dates = [], []
    for _, row in df.iterrows():
        title = row.get("ê·¼ê±° ê¸°ì‚¬ ì œëª©", "")
        date_str, clean_title = extract_date_from_title(title)
        titles.append(clean_title)
        dates.append(date_str or "")

    df = df.copy()
    df["ê·¼ê±° ê¸°ì‚¬ ì œëª©"] = titles
    df["ê¸°ì‚¬ ë‚ ì§œ"] = dates
    return df

def make_prompt(competitor, data_json, business_name=None):
    """ê²½ìŸì‚¬ë³„ í˜‘ë ¥ì‚¬ ì¶”ì¶œì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    if business_name:
        business_text = f"ëŒ€ì›…ê·¸ë£¹ì˜ **'{business_name}'** ì‚¬ì—…ê³¼ ì§ì ‘ì ìœ¼ë¡œ ì—°ê´€ëœ ê²½ìŸì‚¬ì…ë‹ˆë‹¤."
        business_hint = f"CSVì˜ 'ì‚¬ì—…ëª…' ì»¬ëŸ¼ì—ëŠ” ëª¨ë“  í–‰ì—ì„œ **'{business_name}'**ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”."
    else:
        business_text = "ëŒ€ì›…ê·¸ë£¹ê³¼ ì—°ê´€ëœ ê²½ìŸì‚¬ì…ë‹ˆë‹¤."
        business_hint = "ì‚¬ì—…ëª…ì´ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš°, 'ì‚¬ì—…ëª…' ì»¬ëŸ¼ì€ ë¹„ì›Œ ë‘ê±°ë‚˜ ê¸°ì‚¬ ë§¥ë½ìƒ ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¦„ì„ ì‚¬ìš©í•˜ì„¸ìš”."

    prompt = f"""
ë‹¹ì‹ ì€ **ëŒ€ì›…ê·¸ë£¹ì˜ ê²½ìŸì‚¬ ë™í–¥ ë¶„ì„ ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.

ì—¬ê¸°ì„œ ë§í•˜ëŠ” **"{competitor}"**ëŠ” {business_text}
ë™ëª…ì˜ ë‹¤ë¥¸ íšŒì‚¬(ì´ë¦„ë§Œ ê°™ì€ ë‹¤ë¥¸ ê¸°ì—…)ì™€ **ì ˆëŒ€ í˜¼ë™í•˜ì§€ ë§ˆì„¸ìš”.**

ì•„ë˜ì— ì œê³µëœ ê¸°ì‚¬ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„í•´ì•¼ í•˜ë©°,
ë‹¹ì‹ ì´ ì‚¬ì „ì— ì•Œê³  ìˆëŠ” ì¼ë°˜ ì§€ì‹ì´ë‚˜ ì™¸ë¶€ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬
ìƒˆë¡œìš´ ì‚¬ì‹¤(íŒŒíŠ¸ë„ˆì‹­, íšŒì‚¬ëª…, ì„œë¹„ìŠ¤ëª… ë“±)ì„ **ì¶”ê°€ë¡œ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.**

ë°˜ë“œì‹œ ë‹¤ìŒ ì›ì¹™ì„ ì§€í‚¤ì„¸ìš”:

1. **ê¸°ì‚¬ ë³¸ë¬¸ì— ì‹¤ì œë¡œ ë“±ì¥í•˜ëŠ” ì •ë³´ë§Œ ì‚¬ìš©**
2. **'{competitor}'ì™€ì˜ ì§ì ‘ì ì¸ ê´€ê³„ë§Œ íŒŒíŠ¸ë„ˆì‹­ìœ¼ë¡œ ì¸ì •**
3. **ëŒ€ì›… ì‚¬ì—… ê´€ì  ìš°ì„ **

[ë¶„ì„ìš© ê¸°ì‚¬ ë°ì´í„°(JSON)]
{data_json}

ì¶œë ¥ í˜•ì‹ ìš”êµ¬ ì‚¬í•­ (ë§¤ìš° ì¤‘ìš”):

1. ì¶œë ¥ì€ **ìˆœìˆ˜ CSV í…ìŠ¤íŠ¸ë§Œ** í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
   - ì½”ë“œ ë¸”ë¡, ì„¤ëª… ë¬¸ì¥, ì£¼ì„, ì¸ìš©êµ¬, ë§ˆí¬ë‹¤ìš´ í‘œ ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
   - ì˜¤ì§ CSV í–‰ë“¤ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

2. ì²« ë²ˆì§¸ ì¤„ì€ ë°˜ë“œì‹œ **í—¤ë”**ë¡œ ì•„ë˜ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
   - ë²ˆí˜¸,ì‚¬ì—…ëª…,ê²½ìŸì‚¬,í˜‘ë ¥ì‚¬/ê¸°ê´€ëª…,í˜‘ë ¥ ìœ í˜•,ê·¼ê±° ê¸°ì‚¬ ì œëª©,ê·¼ê±° ê¸°ì‚¬ URL

3. ê° ë°ì´í„° í–‰ì€ ì•„ë˜ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
   - ë²ˆí˜¸: ì¼ë ¨ë²ˆí˜¸ (1ë¶€í„° ì‹œì‘). ë¹„ì›Œ ë‘ì–´ë„ ë©ë‹ˆë‹¤.
   - ì‚¬ì—…ëª…: {business_hint}
   - ê²½ìŸì‚¬: '{competitor}'ë¥¼ ê·¸ëŒ€ë¡œ ì…ë ¥í•˜ì„¸ìš”.
   - í˜‘ë ¥ì‚¬/ê¸°ê´€ëª…: '{competitor}'ì™€ ì§ì ‘ì ì¸ íŒŒíŠ¸ë„ˆì‹­/í˜‘ë ¥ ê´€ê³„ì— ìˆëŠ” íšŒì‚¬ ë˜ëŠ” ê¸°ê´€ëª…
   - í˜‘ë ¥ ìœ í˜•: ê¸°ì‚¬ì— ê·¼ê±°í•œ êµ¬ì²´ì ì¸ í˜‘ë ¥ í˜•íƒœ(ì˜ˆ: EAP ë„ì…, ê³µë™ ì—°êµ¬, ê¸°ìˆ  ì—°ë™, íˆ¬ì ìœ ì¹˜, ì„œë¹„ìŠ¤ ë„ì… ë“±)
   - ê·¼ê±° ê¸°ì‚¬ ì œëª©: í•´ë‹¹ íŒŒíŠ¸ë„ˆì‹­ì´ ì–¸ê¸‰ëœ ê¸°ì‚¬ ì œëª© (JSONì˜ "ê¸°ì‚¬ ì œëª©"ì—ì„œ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ê¸°)
   - ê·¼ê±° ê¸°ì‚¬ URL: JSONì— "ê¸°ì‚¬ URL"ì´ ìˆì„ ê²½ìš° ê·¸ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹ˆ ì¹¸ìœ¼ë¡œ ë‚¨ê¹€

4. CSV í˜•ì‹ ì„¸ë¶€ ê·œì¹™:
   - êµ¬ë¶„ìëŠ” ì‰¼í‘œ(,)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
   - í•„ë“œ ì•ˆì— ì‰¼í‘œë‚˜ ì¤„ë°”ê¿ˆì´ ë“¤ì–´ê°€ëŠ” ê²½ìš°ì—ëŠ” ê·¸ í•„ë“œë¥¼ í°ë”°ì˜´í‘œ(")ë¡œ ê°ì‹¸ì„¸ìš”.
   - í—¤ë”ë¥¼ ì œì™¸í•œ ë°ì´í„° í–‰ì´ í•˜ë‚˜ë„ ì—†ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ê·¸ ê²½ìš° í—¤ë”ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ìœ„ ì¡°ê±´ì„ ëª¨ë‘ ì§€í‚¤ë©´ì„œ CSVë¥¼ ì¶œë ¥í•˜ì„¸ìš”.
"""
    return prompt

# ---------------------------
# LLM í˜¸ì¶œ (í•µì‹¬ ìˆ˜ì •)
# ---------------------------
async def call_llm_async(session, semaphore, prompt, batch_info, max_retries=6):
    """
    - RPM/TPM ì œí•œ ì ìš©
    - 429: Retry-After ìš°ì„ , ì—†ìœ¼ë©´ ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„°
    - 5xx: ë°±ì˜¤í”„ í›„ ì¬ì‹œë„
    """
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}

    # ëª¨ë¸ ì„ íƒ: gpt-4o (ê¸°ë³¸ê°’), gpt-4o-mini (ë¹„ìš© ì ˆê°), gpt-3.5-turbo (ìµœëŒ€ ì ˆê°)
    # gpt-4o-mini: ë¹„ìš© 94% ì ˆê°, ì„±ëŠ¥ ì•½ê°„ ì €í•˜ ê°€ëŠ¥
    # gpt-3.5-turbo: ë¹„ìš© 96% ì ˆê°, CSV í˜•ì‹ ì¤€ìˆ˜ ì‹¤íŒ¨ ê°€ëŠ¥ì„± ë†’ìŒ (ë¹„ê¶Œì¥)
    model_name = os.getenv("OPENAI_MODEL", "gpt-4o-mini")  # ê¸°ë³¸ê°’ì„ gpt-4o-minië¡œ ë³€ê²½ (ë¹„ìš© ì ˆê°)
    
    data = {
        "model": model_name,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": 1024,
        "temperature": 0.0
    }

    est_prompt_tokens = estimate_tokens(prompt)
    est_total_tokens = est_prompt_tokens + int(data["max_tokens"])

    for attempt in range(max_retries):
        #  RPM/TPM ì œí•œ: ì—¬ê¸°ì„œ â€œìŠ¤ìŠ¤ë¡œ ê¸°ë‹¤ë¦¬ë©´ì„œâ€ 429ë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ ì¤„ì„
        await rpm_limiter.acquire(1)
        await tpm_limiter.acquire(est_total_tokens)

        async with semaphore:
            try:
                if attempt == 0:
                    print(
                        f"  [ë°°ì¹˜ {batch_info}] LLM ìš”ì²­ ì‹œì‘ "
                        f"(est_tokensâ‰ˆ{est_total_tokens}, RPM={OPENAI_RPM}, TPM={OPENAI_TPM})",
                        flush=True
                    )
                else:
                    print(
                        f"  [ë°°ì¹˜ {batch_info}] ì¬ì‹œë„ {attempt+1}/{max_retries}",
                        flush=True
                    )

                timeout = aiohttp.ClientTimeout(total=OPENAI_TIMEOUT_SEC)

                async with session.post(API_ENDPOINT, headers=headers, json=data, timeout=timeout) as res:
                    if res.status == 429:
                        # Retry-After ìš°ì„ 
                        ra = res.headers.get("Retry-After")
                        if ra:
                            try:
                                wait = float(ra)
                            except ValueError:
                                wait = 30.0
                        else:
                            base = min(60.0, 2.0 ** attempt)
                            wait = base + random.uniform(0.0, 1.5)

                        # ì‘ë‹µ ë°”ë””ì— insufficient_quotaê°€ ìˆìœ¼ë©´ ì¬ì‹œë„ ì˜ë¯¸ ì—†ìŒ (í• ë‹¹ëŸ‰ ì†Œì§„)
                        try:
                            err_json = await res.json()
                            err_info = err_json.get("error", {})
                            err_code = (err_info.get("code") or "").lower()
                            err_message = err_info.get("message", "")
                            
                            if "insufficient_quota" in err_code or "quota" in err_message.lower():
                                print(f"  [ë°°ì¹˜ {batch_info}] âŒ OpenAI API í• ë‹¹ëŸ‰ ì†Œì§„ (insufficient_quota)", flush=True)
                                print(f"  [ë°°ì¹˜ {batch_info}] ğŸ’¡ í•´ê²° ë°©ë²•:", flush=True)
                                print(f"  [ë°°ì¹˜ {batch_info}]    1. OpenAI ê³„ì • ì‚¬ìš©ëŸ‰ í™•ì¸: https://platform.openai.com/usage", flush=True)
                                print(f"  [ë°°ì¹˜ {batch_info}]    2. ê²°ì œ ì •ë³´ í™•ì¸ ë° í¬ë ˆë”§ ì¶©ì „", flush=True)
                                print(f"  [ë°°ì¹˜ {batch_info}]    3. API í‚¤ í™•ì¸ (ì˜¬ë°”ë¥¸ í‚¤ì¸ì§€)", flush=True)
                                return "API í˜¸ì¶œ ì‹¤íŒ¨ (í• ë‹¹ëŸ‰ ì†Œì§„)"
                        except Exception:
                            pass

                        print(f"  [ë°°ì¹˜ {batch_info}] 429 RateLimit â†’ {wait:.1f}s ëŒ€ê¸° í›„ ì¬ì‹œë„", flush=True)
                        await asyncio.sleep(wait)
                        continue

                    if 500 <= res.status < 600:
                        wait = min(60.0, 2.0 ** attempt) + random.uniform(0.0, 1.5)
                        print(f"  [ë°°ì¹˜ {batch_info}] ì„œë²„ ì˜¤ë¥˜ {res.status} â†’ {wait:.1f}s í›„ ì¬ì‹œë„", flush=True)
                        await asyncio.sleep(wait)
                        continue

                    res.raise_for_status()
                    data_json = await res.json()
                    content = data_json["choices"][0]["message"]["content"]
                    return content.strip()

            except asyncio.TimeoutError:
                wait = min(60.0, 2.0 ** attempt) + random.uniform(0.0, 1.5)
                print(f"  [ë°°ì¹˜ {batch_info}] íƒ€ì„ì•„ì›ƒ â†’ {wait:.1f}s í›„ ì¬ì‹œë„", flush=True)
                await asyncio.sleep(wait)

            except aiohttp.ClientResponseError as e:
                if getattr(e, "status", None) == 429:
                    wait = min(60.0, 2.0 ** attempt) + random.uniform(0.0, 1.5)
                    print(f"  [ë°°ì¹˜ {batch_info}] 429(ì˜ˆì™¸) â†’ {wait:.1f}s í›„ ì¬ì‹œë„", flush=True)
                    await asyncio.sleep(wait)
                    continue
                print(f"  [ë°°ì¹˜ {batch_info}] HTTP ì˜¤ë¥˜: {e}", flush=True)
                return "API í˜¸ì¶œ ì‹¤íŒ¨"

            except Exception as e:
                wait = min(30.0, 2.0 ** attempt) + random.uniform(0.0, 1.5)
                print(f"  [ë°°ì¹˜ {batch_info}] ê¸°íƒ€ ì˜¤ë¥˜: {e} â†’ {wait:.1f}s í›„ ì¬ì‹œë„", flush=True)
                await asyncio.sleep(wait)

    print(f"  [ë°°ì¹˜ {batch_info}] ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ â†’ ì‹¤íŒ¨", flush=True)
    return "API í˜¸ì¶œ ì‹¤íŒ¨"

def save_results_to_sheets(results_df, spreadsheet_id, worksheet_name):
    """ê²°ê³¼ë¥¼ Google Sheetsì— ì €ì¥ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€í•˜ê³  ì´ì–´ì„œ ì¶”ê°€)"""
    try:
        print(f"[ì €ì¥] ì‹œì‘: ì‹œíŠ¸='{worksheet_name}', í–‰ ìˆ˜={len(results_df)}, ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID={spreadsheet_id}", flush=True)
        
        client = get_google_client()
        print(f"[ì €ì¥] Google í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ", flush=True)
        
        spreadsheet = client.open_by_key(spreadsheet_id)
        print(f"[ì €ì¥] ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—´ê¸° ì„±ê³µ: {spreadsheet.title}", flush=True)
        
        try:
            worksheet = spreadsheet.worksheet(worksheet_name)
            existing_data = worksheet.get_all_values()
            has_header = len(existing_data) > 0
            print(f"[ì €ì¥] ê¸°ì¡´ ì‹œíŠ¸ ì‚¬ìš© (ê¸°ì¡´ í–‰: {len(existing_data)})", flush=True)
        except Exception as e:
            print(f"[ì €ì¥] ìƒˆ ì‹œíŠ¸ ìƒì„± ì‹œë„: {e}", flush=True)
            worksheet = spreadsheet.add_worksheet(
                title=worksheet_name, rows=1000, cols=10
            )
            has_header = False
            print(f"[ì €ì¥] ìƒˆ ì‹œíŠ¸ ìƒì„± ì™„ë£Œ", flush=True)
        
        output_cols = ["ì‚¬ì—…ëª…", "ê²½ìŸì‚¬", "í˜‘ë ¥ì‚¬/ê¸°ê´€ëª…", "í˜‘ë ¥ ìœ í˜•", "ê·¼ê±° ê¸°ì‚¬ ì œëª©", "ê·¼ê±° ê¸°ì‚¬ URL", "ê¸°ì‚¬ ë‚ ì§œ"]
        
        # í—¤ë”ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
        if not has_header:
            worksheet.append_row(output_cols)
            print(f"[ì €ì¥] í—¤ë” ì¶”ê°€ ì™„ë£Œ", flush=True)
        
        # ë°ì´í„° ì¶”ê°€ (ê¸°ì¡´ ë°ì´í„° ì•„ë˜ì— ì´ì–´ì„œ)
        saved_count = 0
        for idx, (_, row) in enumerate(results_df.iterrows(), 1):
            try:
                row_data = [
                    str(row.get("ì‚¬ì—…ëª…", "")),
                    str(row.get("ê²½ìŸì‚¬", "")),
                    str(row.get("í˜‘ë ¥ì‚¬/ê¸°ê´€ëª…", "")),
                    str(row.get("í˜‘ë ¥ ìœ í˜•", "")),
                    str(row.get("ê·¼ê±° ê¸°ì‚¬ ì œëª©", "")),
                    str(row.get("ê·¼ê±° ê¸°ì‚¬ URL", "")),
                    str(row.get("ê¸°ì‚¬ ë‚ ì§œ", ""))
                ]
                worksheet.append_row(row_data)
                saved_count += 1
                if idx % 10 == 0:
                    print(f"[ì €ì¥] ì§„í–‰: {idx}/{len(results_df)} í–‰ ì €ì¥ë¨", flush=True)
            except Exception as row_error:
                print(f"[ì €ì¥] í–‰ {idx} ì €ì¥ ì‹¤íŒ¨: {row_error}", flush=True)
                continue
        
        print(f"[ì €ì¥] ì™„ë£Œ: {saved_count}/{len(results_df)}ê°œ í–‰ ì €ì¥ë¨", flush=True)
        return saved_count
        
    except Exception as e:
        print(f"[ì €ì¥] ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", flush=True)
        import traceback
        traceback.print_exc()
        raise

def get_already_processed_urls(spreadsheet_id, worksheet_name):
    """ì´ë¯¸ ì²˜ë¦¬ëœ ê¸°ì‚¬ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        client = get_google_client()
        spreadsheet = client.open_by_key(spreadsheet_id)
        worksheet = spreadsheet.worksheet(worksheet_name)
        existing_data = worksheet.get_all_values()

        if len(existing_data) <= 1:
            return set()

        headers = existing_data[0]
        processed_urls = set()

        url_col_idx = None
        for idx, h in enumerate(headers):
            if h.lower() in ("ê·¼ê±° ê¸°ì‚¬ url", "ê·¼ê±°ê¸°ì‚¬url", "url", "ë§í¬"):
                url_col_idx = idx
                break

        if url_col_idx is not None:
            for row in existing_data[1:]:
                if len(row) > url_col_idx and row[url_col_idx]:
                    processed_urls.add(row[url_col_idx].strip())

        return processed_urls
    except Exception:
        return set()

async def process_batch_async(session, semaphore, batch_df, competitor, batch_index, business_name, url_col):
    """ë°°ì¹˜ í•˜ë‚˜ ì²˜ë¦¬"""
    analysis_data = []
    for _, row in batch_df.iterrows():
        # ê¸°ì‚¬ ë³¸ë¬¸ ê¸¸ì´ ì œí•œ (API ì‚¬ìš©ëŸ‰ ê°ì†Œ)
        content = str(row['ë³¸ë¬¸'])[:MAX_ARTICLE_CONTENT_LENGTH]
        if len(str(row['ë³¸ë¬¸'])) > MAX_ARTICLE_CONTENT_LENGTH:
            content += "... (ë³¸ë¬¸ ì¼ë¶€ë§Œ í‘œì‹œë¨)"
        
        item = {
            "ê¸°ì‚¬ ì œëª©": row['ì œëª©'],
            "ê¸°ì‚¬ ë³¸ë¬¸": content,  # ê¸¸ì´ ì œí•œëœ ë³¸ë¬¸ë§Œ ì „ì†¡
        }
        if url_col:
            item["ê¸°ì‚¬ URL"] = row[url_col]
        analysis_data.append(item)

    data_json = json.dumps(analysis_data, ensure_ascii=False, indent=2)
    prompt = make_prompt(competitor, data_json, business_name)

    batch_info = f"{competitor}-{batch_index}"
    csv_text = await call_llm_async(session, semaphore, prompt, batch_info)

    if csv_text in ("API í˜¸ì¶œ ì‹¤íŒ¨", "ì‘ë‹µ ì²˜ë¦¬ ì‹¤íŒ¨"):
        print(f"  [ë°°ì¹˜ ì‹¤íŒ¨] {competitor} ë°°ì¹˜ {batch_index} - LLM í˜¸ì¶œ ì‹¤íŒ¨", flush=True)
        return []

    try:
        csv_text_stripped = csv_text.strip()
        if not csv_text_stripped:
            print(f"  [ë°°ì¹˜ ê²½ê³ ] {competitor} ë°°ì¹˜ {batch_index} - ë¹ˆ CSV", flush=True)
            return []

        if csv_text_stripped.startswith("```"):
            csv_text_stripped = re.sub(r"^```[a-zA-Z]*", "", csv_text_stripped)
            csv_text_stripped = csv_text_stripped.rstrip("`").strip()

        f = StringIO(csv_text_stripped)
        reader = csv.DictReader(f)

        batch_rows = []
        for row in reader:
            llm_title = str(row.get("ê·¼ê±° ê¸°ì‚¬ ì œëª©", "")).strip()

            matched_title = ""
            matched_url = ""
            date_str = None  # ë‚ ì§œëŠ” ì›ë³¸ ì œëª©ì—ì„œ ì¶”ì¶œ

            for _, orig_row in batch_df.iterrows():
                orig_title = str(orig_row.get('ì œëª©', '')).strip()
                if orig_title and llm_title:
                    if llm_title in orig_title or orig_title in llm_title:
                        matched_title = orig_title
                        if url_col:
                            matched_url = str(orig_row.get(url_col, '')).strip()
                        # ì›ë³¸ ì œëª©ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                        date_str, clean_title = extract_date_from_title(orig_title)
                        matched_title = clean_title  # ë‚ ì§œ ì œê±°ëœ ì œëª© ì‚¬ìš©
                        break
                    elif len(llm_title) > 10 and len(orig_title) > 10:
                        if llm_title[:30] in orig_title or orig_title[:30] in llm_title:
                            matched_title = orig_title
                            if url_col:
                                matched_url = str(orig_row.get(url_col, '')).strip()
                            # ì›ë³¸ ì œëª©ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                            date_str, clean_title = extract_date_from_title(orig_title)
                            matched_title = clean_title  # ë‚ ì§œ ì œê±°ëœ ì œëª© ì‚¬ìš©
                            break

            # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ LLM ì œëª© ì‚¬ìš© ë° ë‚ ì§œ ì¶”ì¶œ
            if not matched_title:
                matched_title = llm_title
                date_str, matched_title = extract_date_from_title(matched_title)

            batch_rows.append({
                "ì‚¬ì—…ëª…": business_name or row.get("ì‚¬ì—…ëª…", ""),
                "ê²½ìŸì‚¬": competitor,
                "í˜‘ë ¥ì‚¬/ê¸°ê´€ëª…": row.get("í˜‘ë ¥ì‚¬/ê¸°ê´€ëª…", ""),
                "í˜‘ë ¥ ìœ í˜•": row.get("í˜‘ë ¥ ìœ í˜•", ""),
                "ê·¼ê±° ê¸°ì‚¬ ì œëª©": matched_title,
                "ê·¼ê±° ê¸°ì‚¬ URL": matched_url,
                "ê¸°ì‚¬ ë‚ ì§œ": date_str or "",
            })

        print(f"  [ë°°ì¹˜ ì™„ë£Œ] {competitor} ë°°ì¹˜ {batch_index} - {len(batch_rows)}ê°œ ìˆ˜ì§‘", flush=True)
        return batch_rows

    except Exception as e:
        print(f"  [ë°°ì¹˜ CSV íŒŒì‹± ì˜¤ë¥˜] {competitor} ë°°ì¹˜ {batch_index}: {e}", flush=True)
        import traceback
        traceback.print_exc()
        return []

async def process_competitor_async(session, semaphore, competitor, full_group_df, business_name, url_col):
    """ê²½ìŸì‚¬ í•˜ë‚˜ì˜ ëª¨ë“  ë°°ì¹˜ë¥¼ ì²˜ë¦¬"""
    total_articles = len(full_group_df)
    print(f"\n[ë¶„ì„ ì‹œì‘] ê²½ìŸì‚¬: {competitor} (ì´ {total_articles}ê°œ ê¸°ì‚¬)", flush=True)

    total_batches = (total_articles + ARTICLES_PER_CALL - 1) // ARTICLES_PER_CALL
    all_rows = []

    # (ìˆ˜ì •) íƒœìŠ¤í¬ë¥¼ í•œêº¼ë²ˆì— gatherë¡œ í­ë°œì‹œí‚¤ì§€ ì•Šê³ 
    # in-flight ê°œìˆ˜ë¥¼ ì œí•œí•˜ë©° ìˆœì°¨ì ìœ¼ë¡œ â€œë°œì‚¬â€
    pending = set()
    batch_index = 0

    for start in range(0, total_articles, ARTICLES_PER_CALL):
        end = min(start + ARTICLES_PER_CALL, total_articles)
        batch_df = full_group_df.iloc[start:end].copy()
        batch_index += 1

        print(f"  - ë°°ì¹˜ {batch_index}/{total_batches}: ê¸°ì‚¬ {start+1} ~ {end} ì¤€ë¹„", flush=True)
        task = asyncio.create_task(
            process_batch_async(session, semaphore, batch_df, competitor, batch_index, business_name, url_col)
        )
        pending.add(task)

        if len(pending) >= MAX_BATCH_TASKS_IN_FLIGHT:
            done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
            for d in done:
                try:
                    res = d.result()
                    all_rows.extend(res)
                except Exception as e:
                    print(f"  [ë°°ì¹˜ íƒœìŠ¤í¬ ì˜¤ë¥˜] {e}", flush=True)

    # ë‚¨ì€ íƒœìŠ¤í¬ ìˆ˜ê±°
    if pending:
        done, _ = await asyncio.wait(pending)
        for d in done:
            try:
                res = d.result()
                all_rows.extend(res)
            except Exception as e:
                print(f"  [ë°°ì¹˜ íƒœìŠ¤í¬ ì˜¤ë¥˜] {e}", flush=True)

    # Rate limiterê°€ ìˆìœ¼ë¯€ë¡œ ê¸´ ëŒ€ê¸° ì‹œê°„ ë¶ˆí•„ìš” (ìµœì†Œí•œë§Œ ëŒ€ê¸°)
    # í•„ìš”ì‹œ .envì—ì„œ BATCH_SLEEP_SECONDS ì„¤ì • ê°€ëŠ¥
    batch_sleep = int(os.getenv("BATCH_SLEEP_SECONDS", "5"))
    if batch_sleep > 0:
        await asyncio.sleep(batch_sleep)
    return all_rows

async def main_async():
    print("=" * 60, flush=True)
    print(f"LLM ë¶„ì„ ì‹œì‘", flush=True)
    print(f"ì…ë ¥ ì‹œíŠ¸: '{GS_INPUT_WORKSHEET}'", flush=True)
    print(f"ì¶œë ¥ ì‹œíŠ¸: '{GS_OUTPUT_WORKSHEET}'", flush=True)
    print(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID: {GS_SPREADSHEET_ID}", flush=True)
    print("=" * 60, flush=True)
    print("--- 1. ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ ì‹œì‘ ---", flush=True)
    df_news = get_gsheet_data(GS_SPREADSHEET_ID, GS_INPUT_WORKSHEET)

    if df_news is None or len(df_news) == 0:
        print("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", flush=True)
        return

    url_col = None
    for c in df_news.columns:
        lower = c.lower()
        if lower in ("url", "ë§í¬", "ê¸°ì‚¬url", "ê¸°ì‚¬ url"):
            url_col = c
            break

    print("--- 1-1. ì´ë¯¸ ì²˜ë¦¬ëœ ê¸°ì‚¬ í™•ì¸ ì¤‘ ---", flush=True)
    processed_urls = get_already_processed_urls(GS_SPREADSHEET_ID, GS_OUTPUT_WORKSHEET)
    print(f"ì´ë¯¸ ì²˜ë¦¬ëœ ê¸°ì‚¬: {len(processed_urls)}ê°œ", flush=True)

    if url_col:
        df_news = df_news[~df_news[url_col].isin(processed_urls)].reset_index(drop=True)

    if len(df_news) == 0:
        print("ì²˜ë¦¬í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.", flush=True)
        return

    print(f"ì²˜ë¦¬í•  ìƒˆë¡œìš´ ê¸°ì‚¬: {len(df_news)}ê°œ", flush=True)

    competitor_groups = df_news.groupby('ê²½ìŸì‚¬')
    print(f"ì´ {len(competitor_groups)}ê°œ ê²½ìŸì‚¬ ë°ì´í„° ë¡œë“œ ì™„ë£Œ.", flush=True)

    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)

    #  (ê¶Œì¥) ì»¤ë„¥í„° ì œí•œ/ìºì‹œë¡œ ë„¤íŠ¸ì›Œí¬ ì•ˆì •ì„± í–¥ìƒ
    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT_REQUESTS, ttl_dns_cache=300)

    async with aiohttp.ClientSession(connector=connector) as session:
        total_saved_count = 0
        
        # í—¤ë”ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸ (ì²« ë²ˆì§¸ ê²½ìŸì‚¬ì—ì„œë§Œ í—¤ë” ì²´í¬)
        header_initialized = False
        
        # ê²½ìŸì‚¬ë³„ ìˆœì°¨ ì²˜ë¦¬ ë° ì¦‰ì‹œ ì €ì¥ (ì ì§„ì  ì €ì¥)
        for competitor, full_group_df in competitor_groups:
            business_name = COMPETITOR_BUSINESS_MAP.get(competitor, "")

            competitor_results = await process_competitor_async(
                session, semaphore, competitor, full_group_df, business_name, url_col
            )
            
            # âœ… ê²½ìŸì‚¬ë³„ë¡œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì‹œíŠ¸ì— ì €ì¥
            print(f"\n{'='*60}", flush=True)
            print(f"[ê²½ìŸì‚¬ ì²˜ë¦¬ ì™„ë£Œ] {competitor}: ê²°ê³¼ ê°œìˆ˜={len(competitor_results) if competitor_results else 0}", flush=True)
            print(f"[ê²½ìŸì‚¬ ì²˜ë¦¬ ì™„ë£Œ] {competitor}: ê²°ê³¼ íƒ€ì…={type(competitor_results)}", flush=True)
            if competitor_results:
                print(f"[ê²½ìŸì‚¬ ì²˜ë¦¬ ì™„ë£Œ] {competitor}: ê²°ê³¼ ìƒ˜í”Œ (ì²« 1ê°œ): {competitor_results[0] if len(competitor_results) > 0 else 'ì—†ìŒ'}", flush=True)
            print(f"{'='*60}", flush=True)
            
            if competitor_results and len(competitor_results) > 0:
                try:
                    competitor_df = pd.DataFrame(competitor_results)
                    print(f"[ê²½ìŸì‚¬ ì €ì¥] {competitor}: DataFrame ìƒì„± ì™„ë£Œ", flush=True)
                    print(f"  - í–‰ ìˆ˜: {len(competitor_df)}", flush=True)
                    print(f"  - ì»¬ëŸ¼: {list(competitor_df.columns)}", flush=True)
                    
                    output_cols = ["ì‚¬ì—…ëª…", "ê²½ìŸì‚¬", "í˜‘ë ¥ì‚¬/ê¸°ê´€ëª…", "í˜‘ë ¥ ìœ í˜•", "ê·¼ê±° ê¸°ì‚¬ ì œëª©", "ê·¼ê±° ê¸°ì‚¬ URL", "ê¸°ì‚¬ ë‚ ì§œ"]
                    for col in output_cols:
                        if col not in competitor_df.columns:
                            competitor_df[col] = ""
                            print(f"[ê²½ìŸì‚¬ ì €ì¥] {competitor}: ì»¬ëŸ¼ '{col}' ì¶”ê°€ (ë¹ˆ ê°’)", flush=True)
                    
                    print(f"\n[ê²½ìŸì‚¬ ì €ì¥] {competitor}: ì‹œíŠ¸ì— ì €ì¥ ì‹œì‘", flush=True)
                    print(f"  - ì €ì¥í•  í–‰ ìˆ˜: {len(competitor_df)}", flush=True)
                    print(f"  - ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID: {GS_SPREADSHEET_ID}", flush=True)
                    print(f"  - ì‹œíŠ¸ ì´ë¦„: '{GS_OUTPUT_WORKSHEET}'", flush=True)
                    
                    saved_count = save_results_to_sheets(
                        competitor_df[output_cols], 
                        GS_SPREADSHEET_ID, 
                        GS_OUTPUT_WORKSHEET
                    )
                    
                    total_saved_count += saved_count
                    print(f"\n[ê²½ìŸì‚¬ ì €ì¥ ì™„ë£Œ] {competitor}: {saved_count}ê°œ í–‰ ì €ì¥ë¨ (ëˆ„ì : {total_saved_count}ê°œ)", flush=True)
                    print(f"{'='*60}\n", flush=True)
                    
                except Exception as e:
                    print(f"\n[ê²½ìŸì‚¬ ì €ì¥ ì˜¤ë¥˜] {competitor}: {e}", flush=True)
                    import traceback
                    traceback.print_exc()
                    print(f"{'='*60}\n", flush=True)
            else:
                print(f"[ê²½ìŸì‚¬ ì €ì¥] {competitor}: ê²°ê³¼ê°€ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŒ\n", flush=True)

            # Rate limiterê°€ ìˆìœ¼ë¯€ë¡œ ê¸´ ëŒ€ê¸° ì‹œê°„ ë¶ˆí•„ìš” (ìµœì†Œí•œë§Œ ëŒ€ê¸°)
            competitor_sleep = int(os.getenv("COMPETITOR_SLEEP_SECONDS", "5"))
            if competitor_sleep > 0:
                print(f"[ê²½ìŸì‚¬ ì™„ë£Œ] {competitor} ì™„ë£Œ. {competitor_sleep}ì´ˆ ëŒ€ê¸°", flush=True)
                await asyncio.sleep(competitor_sleep)
            else:
                print(f"[ê²½ìŸì‚¬ ì™„ë£Œ] {competitor} ì™„ë£Œ", flush=True)

    if total_saved_count == 0:
        print("\nì €ì¥ëœ íŒŒíŠ¸ë„ˆì‹­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", flush=True)
    else:
        print(f"\n{'='*50}", flush=True)
        print(f"ì „ì²´ ë¶„ì„ ì™„ë£Œ: ì´ {total_saved_count}ê°œ í–‰ì´ Google Sheets '{GS_OUTPUT_WORKSHEET}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.", flush=True)
        print(f"{'='*50}\n", flush=True)

def main():
    asyncio.run(main_async())

if __name__ == "__main__":
    main()